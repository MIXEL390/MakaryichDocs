[![photo-2025-05-07-11-57-28.jpg](https://i.postimg.cc/CLr9C2R0/photo-2025-05-07-11-57-28.jpg)](https://postimg.cc/YhgXMbND)

# Модернизация и реконфигурирование вычислительного комплекса ФББ МГУ «Макарьич»

**ООО "КОГНИТАР"**  

+7 (499) 899-89-96   
<cognitar@hpc-ai.ru>
2024г

---

## Введение

Этот документ поможет вам быстро разобраться в архитектуре вычислительного комплекса «Макарьич». Он предназначен для специалистов, работающих с системой, и описывает основные этапы работы: подключение, управление пользователями, очередями заданий и виртуализацией.

---

## Сетевая адресация

### 1.1 Внешняя адресация

Выделена внешняя подсеть:  
`93.180.63.0/24`  
Шлюз подсети: `93.180.63.15`

Задействованные адреса:  
- `93.180.63.131` — 1-й шлюз комплекса  
- `93.180.63.132` — 2-й шлюз комплекса  

### 1.2 Внутренняя адресация

- `192.168.0.0/24` — основная сеть  
- `192.168.1.0/24` — сеть управления оборудованием  
- `10.0.0.0/24` — DMZ, сейчас 10G между серверами gs61 и gs62  

---

## Доступ к комплексу

### 2.1 Подключение к узлу управления

После подключения к сети через VPN:  
```bash
ssh login@93.180.63.131  # head01
ssh login@93.180.63.132  # head02
```

Управление кластером осуществляется на узле trinityX.

На нем находятся:

 1. ПО управления кластером TrinityX(Luna)
 2. ПО управления (obol + ldap) и база пользователей
 3. Управление и база SLURM

### 2.2 Подключение к BMC контроллерам оборудования

Подключение через веб-интерфейс: `http://192.168.1.X`

(X – адрес устройства, см. таблицу IP-адресов)

---
## 3 Система виртуализации Proxmox

Proxmox Backup Server (PBS) — клиент-серверное решение для резервного копирования и восстановления виртуальных машин, контейнеров и данных с физических узлов. Решение оптимизировано для проекта Proxmox VE (PVE). PBS поддерживает инкрементное резервное копирование с полной дедупликацией, что значительно снижает нагрузку на сеть и экономит пространство для хранения.

Клиент резервного копирования использует API для доступа к резервным копиям. С помощью инструмента командной строки proxmox-backup-client можно создавать резервные копии и восстанавливать данные (в PVE клиент встроен).

Для управления настройкой резервного копирования и резервными копиями используется веб-интерфейс. Все административные задачи можно выполнять в веб-браузере. Веб-интерфейс также предоставляет встроенную консоль.


Proxmox Virtual Environment – платформа с открытым исходным кодом для управления серверами виртуализации. Proxmox позволяет запускать и управлять виртуальными машинами. Это удобно, если вам нужно развернуть несколько серверов на одном физическом оборудовании.

Как войти в систему Proxmox:
 1. Откройте браузер и перейдите по адресу: ```
 https://<узел проксмокса>:8006```
  2. Введите логин и пароль администратора (root).
  3. После входа можно управлять виртуальными машинами через веб-интерфейс.
Резервное копирование осуществляется через Proxmox Backup Server (PBS)
[![photo-2025-05-07-11-36-00.jpg](https://i.postimg.cc/cLXyzJ9p/photo-2025-05-07-11-36-00.jpg)](https://postimg.cc/rD0P0qLj)

Система резервного копирования Proxmox

Proxmox Backup Server (PBS) — клиент-серверное решение для резервного копирования и восстановления виртуальных машин, контейнеров и данных с физических узлов. Решение оптимизировано для проекта Proxmox VE (PVE). PBS поддерживает инкрементное резервное копирование с полной дедупликацией, что значительно снижает нагрузку на сеть и экономит пространство для хранения.
Клиент резервного копирования использует API для доступа к резервным копиям. С помощью инструмента командной строки proxmox-backup-client можно создавать резервные копии и восстанавливать данные (в PVE клиент встроен).
Для управления настройкой резервного копирования и резервными копиями используется веб-интерфейс. Все административные задачи можно выполнять в веб-браузере. Веб-интерфейс также предоставляет встроенную консоль.

[![photo-2025-05-07-11-37-25.jpg](https://i.postimg.cc/FHnCcknH/photo-2025-05-07-11-37-25.jpg)](https://postimg.cc/YvYxB0H5)

---
## 4  Управление пользователями кластера TrinityX

TrinityX позволяет управлять пользователями и назначать им роли в кластере. Это важно, если в системе работает несколько человек.

Управление пользователями в TrinityX осуществляется с помощью утилиты Obol. Obol представляет собой простую обёртку над командами LDAP для обновления локального LDAP-каталога, установленного по умолчанию. Утилита поддерживает управление как пользователями, так и группами, почти так же, как это выполняется стандартными утилитами Linux.

**Как войти в TrinityX:**

1. Подключитесь к системе через SSH:

`ssh login@trinityX`

1. Введите свои учетные данные.
2. После входа можно управлять пользователями и заданиями кластера.

- Создание изменение пользователя

- `obol user add|modify` …

- Управление группами осуществляется аналогичным образом с использованием:

- `obol group [command]` …_

### - Добавление пользователя с указанием дополнительной информации:

- `obol useradd &lt;имя_пользователя&gt; --fullname "Имя Фамилия" --email "email@example.com"`

### - Экспорт данных о пользователях в файл:

- `obol export users --output &lt;имя_файла&gt;`

### - Импорт данных о пользователях из файла:

- `obol import users --file &lt;имя_файла&gt;`

---
Создание пользователя кластера:
[![photo-2025-05-07-11-22-22.jpg](https://i.postimg.cc/6pVFvQx7/photo-2025-05-07-11-22-22.jpg)](https://postimg.cc/Yv9XcM9k)
---
## 5 Система динамической загрузки модулей modules

Modules — это инструмент, упрощающий инициализацию оболочки и позволяющий пользователям легко изменять своё окружение во время сессии с использованием modulefiles.

Примеры:

- Пример загрузки модуля на машине с Linux под Bash:
```
- $ module load gcc/9.4.0
- $ which gcc
- $ /usr/local/gcc/9.4.0/linux-x86_64/bin/gcc
- Переключение между версиями модуля:
- $ module switch gcc gcc/10
- $ which gcc
- /usr/local/gcc/10.3.0/linux-x86_64/bin/gcc
- Выгрузка модуля:
- $ module unload gcc
- $ which gcc
- gcc not found
```

1. Введите команду `squeue`, чтобы проверить состояние задач.
2. Чтобы запустить задание, используйте `sbatch`.

Создано 2 очереди (partition): gpu для узлов с GPU, cpu
с узлами CPU. Доступ к очереди определяется наличием пользователя в одноименной группе.

[![photo-2025-05-07-11-40-33.jpg](https://i.postimg.cc/bvk0JZ7M/photo-2025-05-07-11-40-33.jpg)](https://postimg.cc/wtxR4jTc)
---
## 6 Система очередей SLURM

SLURM помогает распределять вычислительные ресурсы между пользователями. Это избавляет вас от необходимости вручную искать доступные узлы.

Установлена система пакетной обработки заданий slurm версии 22.05.9

Как войти в SLURM:

1.	Подключитесь к серверу управления через SSH:
`ssh login@head01\02`
2.	Введите команду `squeue`, чтобы проверить состояние задач.
3.	Чтобы запустить задание, используйте `sbatch`.

По умолчанию, если не указана партиция запуска (опция -p), то задание попадает в очередь cpu.

- Отправка задания в очередь
- По умолчанию в очередь cpu:
- `_sbatch my_job_script.sh_`
- Явно указать очередь gpu:
- `_sbatch -p gpu my_job_script.sh_`
- Запуск интерактивной сессии:
- В очередь cpu:
- `_srun --pty bash_`
- В очередь gpu:
- `_srun -p gpu --pty bash_`
- Мониторинг очереди и заданий
- Просмотр очереди и состояния заданий:
- `_squeue_`
- Фильтрация по пользователю:
- `_squeue -u your_username_`
- Просмотр задания в конкретной очереди:
- В очереди cpu:
- `_squeue -p cpu_`
- В очереди gpu:
- `_squeue -p gpu_`
- Отмена задания
- Отмена задания по его ID:
- `_scancel &lt;job_id&gt;_`
- Отмена всех заданий пользователя:
- `_scancel -u your_username_`

---
## 7 Система управления TrinityX

TrinityX — это новое поколение открытого программного обеспечения для управления инфраструктурой HPC и AI от компании ClusterVision, теперь с расширенными возможностями CloudBursting. Кроме того, TrinityX включает дополнительные модули, адаптированные к специфическим потребностям, что позволяет точно настраивать вычислительные узлы.

Используются 3 группы узлов: cpu, gpu, service.
[![photo-2025-05-07-11-42-47.jpg](https://i.postimg.cc/c193d7KC/photo-2025-05-07-11-42-47.jpg)](https://postimg.cc/nMmrkDGf)

``` bash
Добавление узла luna в конфигурацию кластера:

- _luna node add --group cpu &lt;newnode&gt;_

Снять образ ОС с узла

- _luna node osgrab -o cpu node01 -no_

Собрать образ ОС для использования

- _luna osimage pack cpu_
```

Крайне полезен в повседневной работе параллельный интерпретатор команд `pdsh` (группа или имя узла или список)

Например, выполнить команду `hostname` на некоторых серверах:

В комплексе используется 2 типа общего межузлового доступа к данным:

[![image.png](https://i.postimg.cc/cC26wmYc/image.png)](https://postimg.cc/Fkb9Q0Tf)

`/data` – NFS, хранит домашние папки пользователей, на data01

`/shared` – используется для хранения системного программного обеспечения и общего окружения, хранится на управляющем узле.

---
## Приложения


| **Комната** | **146а** |
| --- | --- |
| **Адрес** | 93.180.63.131 (head01)93.180.63.132 (head02) |
| **DNS** | makarich.fbb.msu.ru -> 131 ma.fbb.msu.ru -> 132 |
| **Настройки сети** | GW: 93.180.63.15 DNS: 93.180.63.1 8.8.8.8 Mask:255.255.255.0 |

### _Таблица 2. Адреса в сети обмена данными_

| Адрес | Сервер | Примечание |
| --- | --- | --- |
| 192.168.0.1-32 | node [ 01-32 ] | cpu |
| --- | --- | --- |
| 192.168.0.61 | gs61 | proxmox |
| --- | --- | --- |
| 192.168.0.62 | gs62 | proxmox |
| --- | --- | --- |
| 192.168.0.100 | head01 | proxmox |
| --- | --- | --- |
| 192.168.0.101 | head02 | proxmox |
| --- | --- | --- |
| 192.168.0.200 | trinityX | manager VIRT |
| --- | --- | --- |
| 192.168.0.201 | head01 | service VIRT |
| --- | --- | --- |
| 192.168.0.202 | head02 | service VIRT |
| --- | --- | --- |
| 192.168.0.161 | gs161 | gpu VIRT |
| --- | --- | --- |
| 192.168.0.162 | gs162 | gpu VIRT |
| --- | --- | --- |
| 192.168.0.246 | data01 | proxmox NFS |
| --- | --- | --- |
| 192.168.0.251 | gw1 | gateway VIRT |
| --- | --- | --- |
| 192.168.0.252 | gw2 | gateway VIRT |

### _Таблица 3. Адреса в служебной сети_

| Адрес | Сервер | Примечание |
| --- | --- | --- |
| 192.168.1.3-32 | nodeN | admin uhu,pry |
| --- | --- | --- |
| 192.168.1.50 | UPS кластера |     |
| --- | --- | --- |
| 192.168.1.61 | gs61 | ADMIN QGKPNACUQF |
| --- | --- | --- |
| 192.168.1.62 | gs62 | ADMIN QJZATWSCZH |
| --- | --- | --- |
| 192.168.1.100 | head01 | admin uhu,pry |
| --- | --- | --- |
| 192.168.1.101 | head02 | admin uhu,pry |
| --- | --- | --- |
| 192.168.1.246 | data01 | login: root password: 0penBmc (Zero!) |
| --- | --- | --- |
| 192.168.1.249 | Dlink DGS-1210-28T | admin admin |
| 192.168.1.250 | 3COM 4510G 48p 1G | admin uhu,pry |
| --- | --- | --- |
| 192.168.1.252 | FC  |     |
| 192.168.1.253 | eltex |     |
| 192.168.1.254 | 3COM 2250 48p 100m | admin uhu,pry |

### _Таблица 4. MAC-Адреса_

| **Node** | **eth0** | **eth1** | **BMC** |
| --- | --- | --- | --- |
| node03 | F4:6D:04:54:98:DA | f4:6d:04:54:98:d9 | 00:24:8c:99:6b:46 (manual set) |
| node04 | F4:6D:04:54:98:7A | f4:6d:04:54:98:79 | f4:6d:04:2c:ee:c5 |
| node05 | F4:6D:04:54:98:82 | f4:6d:04:54:98:81 | f4:6d:04:2c:ee:cc |
| node06 | F4:6D:04:54:99:B8 | f4:6d:04:54:99:b7 | f4:6d:04:2c:ee:73 |
| node07 | F4:6D:04:54:98:D2 | f4:6d:04:54:98:d1 | f4:6d:04:2c:ee:d3 |
| node08 | F4:6D:04:54:98:D6 | f4:6d:04:54:98:d5 | f4:6d:04:2c:ee:c8 |
| node09 | f4:6d:04:54:98:9c | f4:6d:04:54:98:9b | f4:6d:04:2c:ef:29 |
| node10 | F4:6D:04:54:98:BC | f4:6d:04:54:98:bb | f4:6d:04:2c:ee:a1 |
| node11 | F4:6D:04:54:98:DE | f4:6d:04:54:98:dd | f4:6d:04:2c:ee:82 |
| node12 | F4:6D:04:54:99:BC | f4:6d:04:54:99:bb | f4:6d:04:2c:ee:6f |
| node13 | f4:6d:04:54:98:1c | f4:6d:04:54:98:1b | f4:6d:04:2c:ef:2d |
| node14 | f4:6d:04:54:98:20 | f4:6d:04:54:98:1f | f4:6d:04:2c:ee:d6 |
| node15 | f4:6d:04:54:99:b4 | f4:6d:04:54:99:b3 | f4:6d:04:2c:ee:bf |
| node16 | f4:6d:04:54:99:48 | f4:6d:04:54:99:47 | f4:6d:04:2c:ee:6a |
| node17 | f4:6d:04:54:98:ac | f4:6d:04:54:98:ab | f4:6d:04:2c:ee:a9 |
| node18 | f4:6d:04:54:98:a8 | f4:6d:04:54:98:a7 | f4:6d:04:2c:ee:cd |
| node19 | f4:6d:04:54:99:76 | f4:6d:04:54:99:75 | f4:6d:04:2c:ee:bc |
| node20 | f4:6d:04:54:98:22 | f4:6d:04:54:98:21 | f4:6d:04:2c:ee:c7 |
| node21 | f4:6d:04:54:98:76 | f4:6d:04:54:98:75 | f4:6d:04:2c:ee:77 |
| node22 | f4:6d:04:54:98:48 | f4:6d:04:54:98:47 | 00:10:20:3d:4e:88 |
| node23 | f4:6d:04:54:98:8a | f4:6d:04:54:98:89 | f4:6d:04:2c:ee:be |
| node24 | f4:6d:04:54:98:84 | f4:6d:04:54:98:83 | f4:6d:04:2c:ef:2c |
| node25 | f4:6d:04:54:99:ac | f4:6d:04:54:99:ab | f4:6d:04:2c:ee:d5 |
| node26 | f4:6d:04:54:99:9e | f4:6d:04:54:99:9d | f4:6d:04:2c:ee:c0 |
| node27 | f4:6d:04:54:99:0c | f4:6d:04:54:99:0b | f4:6d:04:2c:ee:c9 |
| node28 | f4:6d:04:54:98:7c | f4:6d:04:54:98:7b | f4:6d:04:2c:ee:96 |
| node29 | f4:6d:04:54:99:96 | f4:6d:04:54:99:95 | f4:6d:04:2c:ee:c1 |
| node30 | f4:6d:04:54:98:d8 | f4:6d:04:54:98:d7 | 00:10:20:3d:4e:88 |
| node31 | f4:6d:04:54:99:b2 | f4:6d:04:54:99:b1 | f4:6d:04:2c:ee:a6 |
| node32 | f4:6d:04:54:98:56 | f4:6d:04:54:98:55 | f4:6d:04:2c:ee:a2 |

### Блок-схемы работы кластера

[![image.png](https://i.postimg.cc/jdfSByPG/image.png)](https://postimg.cc/F79vLdPZ)
[![image.png](https://i.postimg.cc/nL3Fq2fQ/image.png)](https://postimg.cc/p9nMHDT2)
### Блок схема стека программного обеспечения кластера

[![image.png](https://i.postimg.cc/NM00nVk4/image.png)](https://postimg.cc/Z91ZBwgB)

ООО "КОГНИТАР"

+7 (499) 899-89-96   <cognitar@hpc-ai.ru>
